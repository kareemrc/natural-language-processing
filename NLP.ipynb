{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.6 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "2db524e06e9f5f4ffedc911c917cb75e12dbc923643829bf417064a77eb14d37"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "lines = []\n",
    "\n",
    "with codecs.open('tereIshqKiIntiha.txt','r',encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        lines.append(line.strip().split())\n",
    "        \n",
    "p=lines\n",
    "rev=[]\n",
    "data=[]\n",
    "tmp=''\n",
    "for i in lines:\n",
    "    #i.insert(0,\".\")\n",
    "    #rev.append(i[::-1])\n",
    "    i.reverse()\n",
    "    rev.append(i)\n",
    "    for j in i:\n",
    "        tmp=j[::-1]\n",
    "        data.append(tmp)\n",
    "    \n",
    "tokens=data\n",
    "lines=rev\n",
    "\n",
    "l,s=[],[]\n",
    "for i in lines:\n",
    "    l = [i[::-1] for i in i]\n",
    "    s.append(l)\n",
    "\n",
    "lines=s\n",
    "lines1=[]\n",
    "seq=\"\"\n",
    "for i in range(9,len(tokens)):\n",
    "    seq=tokens[i-9:i]\n",
    "    line=\" \".join(seq)\n",
    "    lines1.append(line)\n",
    "\n",
    "#print(len(lines))\n",
    "print(lines1)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(lines1)\n",
    "sequences = tokenizer.texts_to_sequences(lines1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = np.array(sequences)\n",
    "\n",
    "X,y=[],[]\n",
    "# for i in range(0,len(sequences)):\n",
    "#     X.append(sequences[i][:-2])\n",
    "#     y.append(sequences[i][-2])\n",
    "\n",
    "X,y= sequences[:,:-1],sequences[:,-1]\n",
    "X[0]\n",
    "X=np.array(X)\n",
    "y=np.array(y)\n",
    "\n",
    "seq_lenght=X.shape[1]\n",
    "print(y.shape, y.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size=len(tokenizer.word_index)+1\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = to_categorical(y, num_classes=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 15, input_length=seq_length))\n",
    "model.add(LSTM(20, return_sequences=True))\n",
    "model.add(LSTM(20))\n",
    "model.add(Dense(20, activation='relu'))\n",
    "model.add(Dense(vocab_size, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X, y, batch_size = 15, epochs = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_text=\"کوئی دم کا کا سامنا دیکھ کیا راز کی\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_seq(model, tokenizer, text_seq_length, seed_text, n_words):\n",
    "  text = []\n",
    "\n",
    "  for _ in range(n_words):\n",
    "    encoded = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "    encoded = pad_sequences([encoded], maxlen = text_seq_length, truncating='pre')\n",
    "\n",
    "    y_predict = model.predict_classes(encoded)\n",
    "\n",
    "    predicted_word = ''\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "      if index == y_predict:\n",
    "        predicted_word = word\n",
    "        break\n",
    "    seed_text = seed_text + ' ' + predicted_word\n",
    "    text.append(predicted_word)\n",
    "  return ' '.join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'ےب ےب ےب ےب ہک ہک ےب ےب ہک ےب'"
      ]
     },
     "metadata": {},
     "execution_count": 165
    }
   ],
   "source": [
    "OP=generate_text_seq(model, tokenizer, seq_length, seed_text, 10)\n",
    "OP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "file1 = codecs.open(\"output.txt\", \"w+\", \"utf-8\")\n",
    "\n",
    "# l,s=[],\"\"\n",
    "# tokens.reverse()\n",
    "# for i in tokens:\n",
    "#     l = i[::-1]\n",
    "#     s+=l+\" \"\n",
    "s = OP[::-1]\n",
    "o=s\n",
    "file1.write(o)\n",
    "file1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}